{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a49e7a53",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import accumulate\n",
    "from omegaconf import OmegaConf\n",
    "from run_utils import get_etab, optimize_sequence, string_to_int, process_configs, cat_neighbors_nodes, rewrite_pdb_sequences, chain_to_partition_map, inter_partition_contact_mask\n",
    "from potts_mpnn_utils import PottsMPNN, tied_featurize, nlcpl, parse_PDB, parse_PDB_seq_only, loss_nll\n",
    "import etab_utils as etab_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dab4b263",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded. Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Define the configuration dictionary\n",
    "# REPLACE paths and values with your specific requirements\n",
    "config_dict = {\n",
    "    \"dev\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    \"out_dir\": \"outputs/example_sequence_outputs\",\n",
    "    \"out_name\": \"sample_run\",\n",
    "    \"input_list\": \"inputs/example_list_sample_seqs.txt\",  # Path to a text file containing list of PDBs and optionally chain information (e.g., 1abc designs all chains for pdb 1abc, 1abc|A:B|C designs chains A and B with visible chain C, 1abc|A:B:C| designs chains A, B, and C\n",
    "    \"input_dir\": \"inputs/example_pdbs\",  # Directory containing .pdb files\n",
    "    \"chain_dict_json\": None, # Dictionary with lists of designed and visible chains for each pdb (only needed if specific chains need to be designed and info is not in input_list\n",
    "    \"model\": { # Model configs (change check_path as needed, don't change others unless using a retrained model)\n",
    "        \"check_path\": \"vanilla_model_weights/pottsmpnn_msa_20.pt\",\n",
    "        \"hidden_dim\": 128,\n",
    "        \"edge_features\": 128,\n",
    "        \"potts_dim\": 400,\n",
    "        \"num_layers\": 3,\n",
    "        \"num_edges\": 48,\n",
    "    },\n",
    "    \"inference\": {\n",
    "        \"num_samples\": 1, # Number of sequences to sample for each structure (must be 1 if optimizing)\n",
    "        \"temperature\": 0.1, # Sampling temperature (default 0.1)\n",
    "        \"noise\": 0.0, # Noise to add during inference (default 0.0)\n",
    "        \"skip_gaps\": False, # Whether to skip gaps in structure (default False, but for some downstream applications like forward folding it can be convenient to set to True)\n",
    "        \"fix_decoding_order\": True, # Whether to use a fixed decoding order for each structure\n",
    "        \"decoding_order_offset\": 0, # Offset if you want a fixed decoding order different from the standard fixed decoding order\n",
    "        \"optimization_mode\": \"potts\", # Optimization protocol (default \"potts\"); for no optimization set to \"none\"; for node optimization set to \"nodes\"\n",
    "        \"optimization_temperature\": 0.0, # Optimization temperature (default 0.1)\n",
    "        \"binding_energy_optimization\": \"none\", # Indicates how to incorporate binding energies into optimization (default \"none\"); for joint binding and stability optimization use \"both\"; for only binding use \"only\"\n",
    "        \"binding_energy_json\": None, # \"inputs/example_binding_energy_partitions_seqs.json\", # Path to json with information about how chains should be separated for binding energy calculation for optimization (required for binding energy optimization)\n",
    "        \"binding_energy_cutoff\": 8, # Angstrom cutoff for which residues to optimize with binding energies (binding energies only accurate for residues close to the interface)\n",
    "        \"optimize_pdb\": False, # Indicates that sequences in .pdb files should be optimized.\n",
    "        \"optimize_fasta\": \"\", # Indicates that sequences in the input .fasta file should be optimized (must have headers matching pdb_list)\n",
    "        \"write_pdb\": True, # Indicates that .pdb files with the best sequence for each structure should be created\n",
    "        \"fixed_positions_json\": \"\", # Filename for a json containing fixed positions (1 indexed list of positions in the structure) for each chain in each structure\n",
    "        \"pssm_json\": \"\", # Filename for a json containing fixed positions (1 indexed list of positions in the structure) for each chain in each structure\n",
    "        \"omit_AA_json\": \"\", # Filename for a json containing pssm info (coefficients of chain length and bias of shape chain length by vocab (21 for standard model, 22 for MSA model)) for each chain in each structure\n",
    "        \"bias_AA_json\": \"\", # Filename for a json with residue bias info (vocab)\n",
    "        \"tied_positions_json\": \"\", # Filename for a json containing tied position information for each structure\n",
    "        \"bias_by_res_json\": \"\", # Filename for a json containing residue bias info (shape chain length by vocab) for each chain in each structure\n",
    "        \"omit_AAs\": [], # List of residues to omit from prediction\n",
    "        \"pssm_threshold\": 0.0, # A value between -inf + inf to restrict per position AAs\n",
    "        \"pssm_multi\": 0.0, # A value between [0.0, 1.0], 0.0 means do not use pssm, 1.0 ignore MPNN predictions\n",
    "        \"pssm_log_odds_flag\": False, # 0 for False, 1 for True\n",
    "        \"pssm_bias_flag\": False # 0 for False, 1 for True\n",
    "    }\n",
    "}\n",
    "config_dict[\"model\"][\"vocab\"] = 22 if 'msa' in config_dict[\"model\"][\"check_path\"] else 21\n",
    "if config_dict[\"inference\"][\"temperature\"] == 0: config_dict[\"inference\"][\"temperature\"] = 1e-6\n",
    "if config_dict[\"inference\"][\"optimization_temperature\"] == 0: config_dict[\"inference\"][\"optimization_temperature\"] = 1e-6\n",
    "if config_dict[\"inference\"][\"optimize_pdb\"] or config_dict[\"inference\"][\"optimize_fasta\"]: config_dict[\"inference\"][\"num_samples\"] = 1\n",
    "if config_dict[\"inference\"][\"optimization_mode\"] == \"none\": config_dict[\"inference\"][\"optimization_mode\"] = \"\"\n",
    "\n",
    "# Convert dictionary to an OmegaConf object for dot-notation access (cfg.model.vocab)\n",
    "cfg = OmegaConf.create(config_dict)\n",
    "dev = cfg.dev\n",
    "print(f\"Configuration loaded. Running on device: {dev}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7e78e2c",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model checkpoint...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading model checkpoint...\")\n",
    "checkpoint = torch.load(cfg.model.check_path, map_location='cpu', weights_only=False) \n",
    "\n",
    "model = PottsMPNN(\n",
    "    ca_only=False, \n",
    "    num_letters=cfg.model.vocab, \n",
    "    vocab=cfg.model.vocab, \n",
    "    node_features=cfg.model.hidden_dim, \n",
    "    edge_features=cfg.model.hidden_dim, \n",
    "    hidden_dim=cfg.model.hidden_dim, \n",
    "    potts_dim=cfg.model.potts_dim, \n",
    "    num_encoder_layers=cfg.model.num_layers, \n",
    "    num_decoder_layers=cfg.model.num_layers, \n",
    "    k_neighbors=cfg.model.num_edges, \n",
    "    augment_eps=cfg.inference.noise\n",
    ")\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "model.eval()\n",
    "model = model.to(dev)\n",
    "pad = (0, 2, 0, 2) # Pad for 'X' and '-' tokens\n",
    "\n",
    "# Freeze parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4c1847",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared to process 6 PDBs.\n"
     ]
    }
   ],
   "source": [
    "def str_split(string, tok):\n",
    "    if not string:\n",
    "        return []\n",
    "    return string.split(tok)\n",
    "\n",
    "# Read list of PDBs to process and derive chain mapping\n",
    "with open(cfg.input_list, 'r') as f:\n",
    "    pdb_lines = f.readlines()\n",
    "pdb_count = defaultdict(int)\n",
    "for pdb in pdb_lines:\n",
    "    pdb_name = pdb.strip().split('|')[0]\n",
    "    pdb_count[pdb_name] += 1\n",
    "pdb_list = []\n",
    "chain_dict = {}\n",
    "chain_suffixes = []\n",
    "for pdb in pdb_lines:\n",
    "    pdb = pdb.strip()\n",
    "    pdb_info = pdb.split('|')\n",
    "    if pdb_count[pdb_info[0]] > 1:\n",
    "        assert len(pdb_info) > 1 # If duplicate pdb entries in list, there must be other identifying info provided\n",
    "        chain_suffixes.append('|' + '|'.join(pdb_info[1:]))\n",
    "    else:\n",
    "        chain_suffixes.append('')\n",
    "    pdb_list.append(pdb_info[0]) # pdb name\n",
    "    if len(pdb_info) > 1: # Get designed and fixed chain info\n",
    "        assert len(pdb_info) == 3\n",
    "        chain_dict[pdb_info[0] + chain_suffixes[-1]] = [str_split(pdb_info[1], ':'), str_split(pdb_info[2], ':')]\n",
    "    else: # Set chain_dict to empty list, which means all chains are designed\n",
    "        chain_dict[pdb_info[0] + chain_suffixes[-1]] = [[], []]\n",
    "if cfg.chain_dict_json is not None:\n",
    "    with open(cfg.inference.chain_dict_json, 'r') as f:\n",
    "        chain_dict = json.load(f)\n",
    "\n",
    "# Load various configuration dictionaries\n",
    "fixed_positions_dict, pssm_dict, omit_AA_dict, bias_AA_dict, tied_positions_dict, bias_by_res_dict, omit_AAs_np = process_configs(cfg)\n",
    "constant = torch.tensor(omit_AAs_np, device=dev)\n",
    "# Setup Alphabet and Bias\n",
    "alphabet = 'ACDEFGHIKLMNPQRSTVWYX-'\n",
    "if cfg.model.vocab == 21:\n",
    "    alphabet = alphabet[:-1]  # remove gap character\n",
    "\n",
    "bias_AAs_np = np.zeros(len(alphabet))\n",
    "if bias_AA_dict:\n",
    "    for n, AA in enumerate(alphabet):\n",
    "        if AA in list(bias_AA_dict.keys()):\n",
    "            bias_AAs_np[n] = bias_AA_dict[AA]\n",
    "constant_bias = torch.tensor(bias_AAs_np, device=dev)\n",
    "\n",
    "print(f\"Prepared to process {len(pdb_list)} PDBs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0768447c",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving sequences to filename outputs/example_sequence_test/test_debug.fasta and saving optimized sequences to filename outputs/example_sequence_test/test_debug_optimized_potts.fasta.\n"
     ]
    }
   ],
   "source": [
    "# Ensure output directories exist for sequences and metrics\n",
    "os.makedirs(cfg.out_dir, exist_ok=True)\n",
    "\n",
    "# Filepaths for outputs\n",
    "filename = os.path.join(cfg.out_dir, cfg.out_name + '.fasta')\n",
    "decoding_order_filename = os.path.join(cfg.out_dir, cfg.out_name + '_decoding_order.json')\n",
    "av_loss_filename = os.path.join(cfg.out_dir, cfg.out_name + '_av_loss.csv')\n",
    "if cfg.inference.write_pdb:\n",
    "    pdb_out_dir = os.path.join(cfg.out_dir, cfg.out_name + '_pdbs')\n",
    "\n",
    "# Optimization check logic\n",
    "skip_calc = False\n",
    "\n",
    "if cfg.inference.optimization_mode:\n",
    "    optimized_filename = os.path.join(cfg.out_dir, cfg.out_name + f'_optimized_{cfg.inference.optimization_mode}.fasta')\n",
    "    \n",
    "    if cfg.inference.optimize_fasta:\n",
    "        assert os.path.exists(cfg.inference.optimize_fasta), f\"Tried to optimize sequences in {cfg.inference.optimize_fasta}, but the file does not exist.\"\n",
    "        print(f\"Found existing sequences at {filename}. Loading for optimization...\")\n",
    "        with open(filename, 'r') as f:\n",
    "            seqs_raw = f.readlines()\n",
    "        # Parse .fasta\n",
    "        existing_seqs = {pdb.strip('>').strip(): seq.strip() for pdb, seq in zip(seqs_raw[::2], seqs_raw[1::2])}\n",
    "        print(f'Saving optimized sequences to filename {optimized_filename}.')\n",
    "        skip_calc = True\n",
    "    elif cfg.inference.optimize_pdb:\n",
    "        print(f\"Optimizing existing sequences in pdb files in {cfg.input_dir}. Loading for optimization...\")\n",
    "        existing_seqs = {}\n",
    "        for pdb, chain_info in zip(pdb_list, chain_suffixes):\n",
    "            wt_info = parse_PDB_seq_only(os.path.join(cfg.input_dir, pdb + '.pdb'), skip_gaps=cfg.inference.skip_gaps) # Parse .pdb files\n",
    "            if chain_info:\n",
    "                _, hidden_chains, vis_chains = chain_info.split('|')\n",
    "                chain_order = hidden_chains.split(':') + vis_chains.split(':')\n",
    "                wt_seq = \"\"\n",
    "                for chain in chain_order:\n",
    "                    if chain: wt_seq += wt_info[f'seq_chain_{chain}']\n",
    "                existing_seqs[pdb + chain_info] = wt_seq\n",
    "            else:\n",
    "                existing_seqs[pdb ] = wt_info['seq']\n",
    "        print(f'Saving optimized sequences to filename {optimized_filename}.')\n",
    "        skip_calc = True\n",
    "    else:\n",
    "        print(f'Saving sequences to filename {filename} and saving optimized sequences to filename {optimized_filename}.')\n",
    "\n",
    "    # Set up binding energy optimization if requested\n",
    "    if cfg.inference.binding_energy_optimization in [\"both\", \"only\"]:\n",
    "        assert cfg.inference.binding_energy_json is not None, \"Chain separation information required for binding energy optimization\"\n",
    "        with open(cfg.inference.binding_energy_json, 'r') as f:\n",
    "            binding_energy_chains = json.load(f)\n",
    "        for pdb, chain_info in zip(pdb_list, chain_suffixes):\n",
    "            assert pdb + chain_info in binding_energy_chains, f\"Chain separation information required for {pdb + chain_info} binding energy optimization\"\n",
    "\n",
    "else:\n",
    "    print(f'Saving to filename {filename}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8997148f",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:07,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Containers to accumulate outputs and metrics across PDBs\n",
    "out_seqs = {}\n",
    "opt_seqs = {}\n",
    "best_seqs = {}\n",
    "\n",
    "av_losses = {'pdb': [], 'seq_loss': [], 'nsr': [], 'potts_loss': []}\n",
    "if cfg.inference.optimize_fasta and os.path.exists(decoding_order_filename):\n",
    "    with open(decoding_order_filename, 'r') as f:\n",
    "        decoding_orders = json.load(f)\n",
    "else:\n",
    "    decoding_orders = {}\n",
    "\n",
    "# Iterate over PDBs\n",
    "for i_pdb, pdb in tqdm(enumerate(pdb_list)):\n",
    "    input_pdb = os.path.join(cfg.input_dir, pdb + '.pdb')\n",
    "    pdb_with_chain_suffix = pdb + chain_suffixes[i_pdb]\n",
    "    # Parse PDB\n",
    "    pdb_data = parse_PDB(input_pdb, chain_dict[pdb_with_chain_suffix][0] + chain_dict[pdb_with_chain_suffix][1], skip_gaps=cfg.inference.skip_gaps)\n",
    "    # Featurize\n",
    "    pdb_chain_dict = {pdb: chain_dict[pdb_with_chain_suffix]}\n",
    "    X, S_true, mask, _, chain_mask, chain_encoding_all, _, _, _, _, chain_M_pos, omit_AA_mask, residue_idx, _, tied_pos_list_of_lists_list, pssm_coef, pssm_bias, pssm_log_odds, bias_by_res, tied_beta, chain_lens = tied_featurize(\n",
    "        [pdb_data[0]], dev, pdb_chain_dict, fixed_positions_dict, omit_AA_dict, \n",
    "        tied_positions_dict, pssm_dict, bias_by_res_dict, ca_only=False, vocab=cfg.model.vocab\n",
    "    )\n",
    "    chain_cuts = [0, *accumulate(chain_lens)]\n",
    "\n",
    "    pssm_log_odds_mask = (pssm_log_odds > cfg.inference.pssm_threshold).float()\n",
    "    \n",
    "    # Set seed if required\n",
    "    if cfg.inference.fix_decoding_order:\n",
    "        torch.manual_seed(string_to_int(pdb) + cfg.inference.decoding_order_offset)\n",
    "    \n",
    "    # Run Encoder\n",
    "    h_V, E_idx, h_E, etab = model.run_encoder(X, mask, residue_idx, chain_encoding_all)\n",
    "    \n",
    "    # Skip sampling if we are just optimizing existing sequences\n",
    "    if not skip_calc:\n",
    "        # etab for Potts energy calculations\n",
    "        etab_functional = etab_utils.functionalize_etab(etab.clone(), E_idx)\n",
    "        etab_functional = torch.nn.functional.pad(etab_functional, pad, \"constant\", 0) # Add padding to account for 'X' and '-' tokens\n",
    "        sample_records = []\n",
    "        sample_seq_loss = []\n",
    "        sample_nsr = []\n",
    "        sample_nlcpl = []\n",
    "        \n",
    "        # Sampling Loop\n",
    "        for sidx in range(cfg.inference.num_samples):\n",
    "            if cfg.inference.fix_decoding_order:\n",
    "                torch.manual_seed(string_to_int(pdb) + cfg.inference.decoding_order_offset + sidx)\n",
    "            \n",
    "            randn = torch.randn(chain_mask.shape, device=X.device)\n",
    "\n",
    "            # Decoder\n",
    "            if tied_positions_dict is None:\n",
    "                output_dict, all_probs = model.decoder(\n",
    "                    h_V, E_idx, h_E, randn, S_true, chain_mask, chain_encoding_all, residue_idx, mask=mask, \n",
    "                    temperature=cfg.inference.temperature, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, \n",
    "                    chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, \n",
    "                    pssm_multi=cfg.inference.pssm_multi, pssm_log_odds_flag=bool(cfg.inference.pssm_log_odds_flag), \n",
    "                    pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(cfg.inference.pssm_bias_flag), \n",
    "                    bias_by_res=bias_by_res\n",
    "                )\n",
    "            else:\n",
    "                output_dict, all_probs = model.tied_decoder(\n",
    "                    h_V, E_idx, h_E, randn, S_true, chain_mask, chain_encoding_all, residue_idx, mask=mask, \n",
    "                    temperature=cfg.inference.temperature, omit_AAs_np=omit_AAs_np, bias_AAs_np=bias_AAs_np, \n",
    "                    chain_M_pos=chain_M_pos, omit_AA_mask=omit_AA_mask, pssm_coef=pssm_coef, pssm_bias=pssm_bias, \n",
    "                    pssm_multi=cfg.inference.pssm_multi, pssm_log_odds_flag=bool(cfg.inference.pssm_log_odds_flag), \n",
    "                    pssm_log_odds_mask=pssm_log_odds_mask, pssm_bias_flag=bool(cfg.inference.pssm_bias_flag), \n",
    "                    tied_pos=tied_pos_list_of_lists_list[0], tied_beta=tied_beta, bias_by_res=bias_by_res\n",
    "                )\n",
    "\n",
    "            # Metrics\n",
    "            log_probs = torch.log(all_probs)\n",
    "            mask_for_loss = mask * chain_mask\n",
    "            _, av_seq_loss, nsr = loss_nll(S_true, torch.nan_to_num(log_probs, nan=0.1, posinf=0.1, neginf=0.1), chain_mask)\n",
    "            nsr = torch.sum((nsr * mask_for_loss).float()) / torch.sum(mask_for_loss)\n",
    "            sample_seq_loss.append(av_seq_loss.cpu().item())\n",
    "            sample_nsr.append(nsr.cpu().item())\n",
    "            _, av_nlcpl_loss = nlcpl(etab, E_idx, S_true, chain_mask)\n",
    "            sample_nlcpl.append(av_nlcpl_loss.cpu().item())\n",
    "\n",
    "            # Convert to String\n",
    "            seq_str = \"\".join(etab_utils.ints_to_seq_torch(output_dict['S'][0]))\n",
    "\n",
    "            # Potts Energy\n",
    "            seq_tensor = output_dict['S'][0].unsqueeze(0).to(dtype=torch.int64, device=E_idx.device)\n",
    "            total_energy, _, _ = etab_utils.calc_eners(etab_functional, E_idx, seq_tensor.unsqueeze(1), None)\n",
    "            total_energy = total_energy.squeeze().cpu().item()\n",
    "            \n",
    "            sample_records.append({\n",
    "                'sample_idx': sidx, 'seq': seq_str, 'energy': total_energy, \n",
    "                'decoding_order': output_dict['decoding_order']\n",
    "            })\n",
    "\n",
    "        # Sort and Store\n",
    "        sample_records = sorted(sample_records, key=lambda x: x['energy'])\n",
    "        for k, rec in enumerate(sample_records):\n",
    "            sidx = rec['sample_idx']\n",
    "            sample_suffix = f\"_{sidx}\" if cfg.inference.num_samples != 1 else ''\n",
    "            out_seqs[pdb_with_chain_suffix + sample_suffix] = ':'.join(rec['seq'][a:b] for a, b in zip(chain_cuts, chain_cuts[1:]))\n",
    "\n",
    "            if pdb not in decoding_orders:\n",
    "                decoding_orders[pdb_with_chain_suffix] = {}\n",
    "            if cfg.inference.num_samples == 1:\n",
    "                decoding_orders[pdb_with_chain_suffix] = rec['decoding_order'].squeeze().cpu().numpy().tolist()\n",
    "            else:\n",
    "                decoding_orders[pdb_with_chain_suffix][sample_suffix.split('_')[1]] = rec['decoding_order'].squeeze().cpu().numpy().tolist()\n",
    "\n",
    "            av_losses['pdb'].append(pdb_with_chain_suffix + sample_suffix)\n",
    "            av_losses['seq_loss'].append(sample_seq_loss[sidx])\n",
    "            av_losses['nsr'].append(sample_nsr[sidx])\n",
    "            av_losses['potts_loss'].append(sample_nlcpl[sidx])\n",
    "\n",
    "            if k == 0: # Save best sequence and sample number\n",
    "                best_seqs[pdb_with_chain_suffix] = (out_seqs[pdb_with_chain_suffix + sample_suffix], sidx)\n",
    "\n",
    "    # Optimization Step (Optional)\n",
    "    if cfg.inference.optimization_mode:\n",
    "        # Setup for binding energy optimization\n",
    "        if cfg.inference.binding_energy_optimization in [\"both\", \"only\"]:\n",
    "            partitions = binding_energy_chains[pdb_with_chain_suffix]\n",
    "            # Ensure chains are ordered correctly in partitions\n",
    "            chain_order_indices = {chain: i_chain for i_chain, chain in enumerate(pdb_data[0]['chain_order'])}\n",
    "            index_order_chains = {i_chain: chain for i_chain, chain in enumerate(pdb_data[0]['chain_order'])}\n",
    "            for i_partition, partition in enumerate(partitions):\n",
    "                partition_indices = sorted([chain_order_indices[chain] for chain in partition])\n",
    "                partitions[i_partition] = [index_order_chains[chain_index] for chain_index in partition_indices]\n",
    "                \n",
    "            # Get energy tables for separated chains\n",
    "            partition_etabs = {}\n",
    "            for i_p, partition in enumerate(partitions):\n",
    "                partition_etabs[i_p] = get_etab(model, pdb_data, cfg, partition)\n",
    "\n",
    "            # Define mapping of residues to partition and partition interface mask\n",
    "            partition_index = chain_to_partition_map(chain_encoding_all, pdb_data[0]['chain_order'], partitions)\n",
    "            inter_mask = inter_partition_contact_mask(X[:,:,1], partition_index, cfg.inference.binding_energy_cutoff)\n",
    "        else:\n",
    "            partition_etabs, partition_index, inter_mask = None, None, None\n",
    "        \n",
    "        # Re-encode for optimization context\n",
    "        h_EX_encoder = cat_neighbors_nodes(torch.zeros_like(h_V), h_E, E_idx)\n",
    "        h_EXV_encoder = cat_neighbors_nodes(h_V, h_EX_encoder, E_idx)\n",
    "        \n",
    "        # Optimize sequences associated with this PDB\n",
    "        source_seqs = existing_seqs if skip_calc else out_seqs\n",
    "        \n",
    "        current_pdb_keys = [k for k in source_seqs.keys() if k.startswith(pdb_with_chain_suffix)]\n",
    "        \n",
    "        for key in current_pdb_keys:\n",
    "            seq_to_opt = source_seqs[key].replace(':', '')\n",
    "            suffix_key = key[len(pdb_with_chain_suffix):] if len(key) > len(pdb_with_chain_suffix) else ''\n",
    "            \n",
    "            stored_decoding = None\n",
    "            if pdb in decoding_orders:\n",
    "                if cfg.inference.num_samples == 1:\n",
    "                    decoding_order = decoding_orders[pdb_with_chain_suffix]\n",
    "                else:\n",
    "                    decoding_order = decoding_orders[pdb_with_chain_suffix].get(suffix_key, None)\n",
    "            else:\n",
    "                if cfg.inference.fix_decoding_order:\n",
    "                    if cfg.inference.num_samples != 1:\n",
    "                        suffix_add = int(suffix_key.split('_')[1])\n",
    "                    else:\n",
    "                        suffix_add = 0\n",
    "                    torch.manual_seed(string_to_int(pdb) + cfg.inference.decoding_order_offset + suffix_add)\n",
    "                randn = torch.randn(chain_mask.shape, device=X.device)\n",
    "                decoding_order = torch.argsort((chain_mask+0.0001)*(torch.abs(randn))).squeeze().cpu().numpy().tolist()\n",
    "                if cfg.inference.num_samples == 1:\n",
    "                    decoding_orders[pdb_with_chain_suffix] = decoding_order\n",
    "                else:\n",
    "                    decoding_orders[pdb_with_chain_suffix][suffix_key.split('_')[1]] = decoding_order\n",
    "                \n",
    "            opt_seq = optimize_sequence(\n",
    "                seq_to_opt, etab, E_idx, mask*chain_M_pos, chain_mask, cfg.inference.optimization_mode, \n",
    "                etab_utils.seq_to_ints, cfg.inference.optimization_temperature, constant, constant_bias, \n",
    "                bias_by_res, cfg.inference.pssm_bias_flag, pssm_coef, pssm_bias, cfg.inference.pssm_multi,\n",
    "                cfg.inference.pssm_log_odds_flag, pssm_log_odds_mask, omit_AA_mask, model, h_E, h_EXV_encoder, h_V, \n",
    "                decoding_order=decoding_order, partition_etabs=partition_etabs, partition_index=partition_index,\n",
    "                inter_mask=inter_mask, binding_optimization=cfg.inference.binding_energy_optimization, vocab=cfg.model.vocab\n",
    "            )\n",
    "            opt_seq = etab_utils.ints_to_seq_torch(opt_seq)\n",
    "            opt_seq = ':'.join(opt_seq[a:b] for a, b in zip(chain_cuts, chain_cuts[1:]))\n",
    "            opt_seqs[key] = opt_seq\n",
    "\n",
    "            if cfg.inference.num_samples == 1 or (not skip_calc and int(suffix_key.split('_')[1]) == best_seqs[pdb_with_chain_suffix][1]): # Overwrite best sequence if on appropriate sample\n",
    "                if pdb_with_chain_suffix in best_seqs:\n",
    "                    suffix = best_seqs[pdb_with_chain_suffix][1]\n",
    "                else:\n",
    "                    suffix = 0\n",
    "                best_seqs[pdb_with_chain_suffix] = (opt_seq, suffix)\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "041b55e2",
   "metadata": {
    "tags": [],
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing optimized sequences to outputs/example_sequence_test/test_debug_optimized_potts.fasta...\n",
      "Writing decoding orders to outputs/example_sequence_test/test_debug_decoding_order.json...\n",
      "Writing sampled sequences to outputs/example_sequence_test/test_debug.fasta...\n",
      "Writing metrics to outputs/example_sequence_test/test_debug_av_loss.csv...\n",
      "Writing new .pdb files to outputs/example_sequence_test/test_debug_pdbs\n",
      "All outputs saved.\n"
     ]
    }
   ],
   "source": [
    "# Write Optimized Sequences (if any)\n",
    "if cfg.inference.optimization_mode:\n",
    "    print(f\"Writing optimized sequences to {optimized_filename}...\")\n",
    "    with open(optimized_filename, 'w') as f:\n",
    "        for pdb_name, seq in opt_seqs.items():\n",
    "            f.write('>' + pdb_name + '\\n' + seq + '\\n')\n",
    "\n",
    "# Write Decoding Orders\n",
    "if cfg.inference.optimization_mode or cfg.inference.fix_decoding_order:\n",
    "    if not os.path.exists(decoding_order_filename) or not cfg.inference.optimize_fasta:\n",
    "        print(f\"Writing decoding orders to {decoding_order_filename}...\")\n",
    "        with open(decoding_order_filename, 'w') as f:\n",
    "            json.dump(decoding_orders, f)\n",
    "\n",
    "# Write Sampled Sequences and Metrics (only if sampling occured)\n",
    "if not skip_calc:\n",
    "    print(f\"Writing sampled sequences to {filename}...\")\n",
    "    with open(filename, 'w') as f:\n",
    "        for pdb_name, seq in out_seqs.items():\n",
    "            f.write('>' + pdb_name + '\\n' + seq + '\\n')\n",
    "            \n",
    "    print(f\"Writing metrics to {av_loss_filename}...\")\n",
    "    av_losses_df = pd.DataFrame(av_losses)\n",
    "    av_losses_df.to_csv(av_loss_filename, index=None)\n",
    "\n",
    "# Write new .pdb files (if requested)\n",
    "if cfg.inference.write_pdb:\n",
    "    print(f\"Writing new .pdb files to {pdb_out_dir}\")\n",
    "    rewrite_pdb_sequences(best_seqs, cfg.input_dir, pdb_out_dir)\n",
    "\n",
    "print(\"All outputs saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e5361-3c4f-4c93-9c0b-5095bb0e03ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PottsMPNN",
   "language": "python",
   "name": "pottsmpnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
